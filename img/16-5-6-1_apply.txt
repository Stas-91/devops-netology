Started by user stasik
Obtained Jenkinsfile from git https://github.com/Stas-91/jenkins_test
[Pipeline] Start of Pipeline
[Pipeline] node
Running on Jenkins in /var/lib/jenkins/workspace/terraform-deploy
[Pipeline] {
[Pipeline] stage
[Pipeline] { (Declarative: Checkout SCM)
[Pipeline] checkout
Selected Git installation does not exist. Using Default
The recommended git tool is: NONE
No credentials specified
Cloning the remote Git repository
Cloning repository https://github.com/Stas-91/jenkins_test
 > git init /var/lib/jenkins/workspace/terraform-deploy # timeout=10
Fetching upstream changes from https://github.com/Stas-91/jenkins_test
 > git --version # timeout=10
 > git --version # 'git version 2.34.1'
 > git fetch --tags --force --progress -- https://github.com/Stas-91/jenkins_test +refs/heads/*:refs/remotes/origin/* # timeout=10
 > git config remote.origin.url https://github.com/Stas-91/jenkins_test # timeout=10
 > git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/* # timeout=10
Avoid second fetch
 > git rev-parse refs/remotes/origin/master^{commit} # timeout=10
Checking out Revision 3f990fb5eea091ebf0b9be0e95374f560f988da0 (refs/remotes/origin/master)
 > git config core.sparsecheckout # timeout=10
 > git checkout -f 3f990fb5eea091ebf0b9be0e95374f560f988da0 # timeout=10
Commit message: "update test for DZ 16-5-6"
 > git rev-list --no-walk f73d58047602260af3715cba0ebdcfcd683bd3ea # timeout=10
[Pipeline] }
[Pipeline] // stage
[Pipeline] withEnv
[Pipeline] {
[Pipeline] withCredentials
Masking supported pattern matches of $AWS_SECRET_ACCESS_KEY or $SSH_PUBLIC_KEY or $AWS_ACCESS_KEY or $YC_TOKEN
[Pipeline] {
[Pipeline] stage
[Pipeline] { (Init)
[Pipeline] sh
+ terraform init -backend-config=access_key=**** -backend-config=secret_key=****

[0m[1mInitializing the backend...[0m
[0m[32m
Successfully configured the backend "s3"! Terraform will automatically
use this backend unless the backend configuration changes.[0m
[0m[1mInitializing modules...[0m
Downloading git::https://github.com/udjin10/yandex_compute_instance.git?ref=main for analytics_vm...
- analytics_vm in .terraform/modules/analytics_vm
Downloading git::https://github.com/udjin10/yandex_compute_instance.git?ref=main for marketing_vm...
- marketing_vm in .terraform/modules/marketing_vm
- vpc_dev in modules/vpc
- vpc_prod in modules/vpc

[0m[1mInitializing provider plugins...[0m
- Finding latest version of yandex-cloud/yandex...
- Finding latest version of hashicorp/template...
- Installing yandex-cloud/yandex v0.139.0...
- Installed yandex-cloud/yandex v0.139.0 (unauthenticated)
- Installing hashicorp/template v2.2.0...
- Installed hashicorp/template v2.2.0 (unauthenticated)

Terraform has created a lock file [1m.terraform.lock.hcl[0m to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.[0m

[33m[33mâ•·[0m[0m
[33mâ”‚[0m [0m[1m[33mWarning: [0m[0m[1mIncomplete lock file information for providers[0m
[33mâ”‚[0m [0m
[33mâ”‚[0m [0m[0mDue to your customized provider installation methods, Terraform was forced
[33mâ”‚[0m [0mto calculate lock file checksums locally for the following providers:
[33mâ”‚[0m [0m  - hashicorp/template
[33mâ”‚[0m [0m  - yandex-cloud/yandex
[33mâ”‚[0m [0m
[33mâ”‚[0m [0mThe current .terraform.lock.hcl file only includes checksums for
[33mâ”‚[0m [0mlinux_amd64, so Terraform running on another platform will fail to install
[33mâ”‚[0m [0mthese providers.
[33mâ”‚[0m [0m
[33mâ”‚[0m [0mTo calculate additional checksums for another platform, run:
[33mâ”‚[0m [0m  terraform providers lock -platform=linux_amd64
[33mâ”‚[0m [0m(where linux_amd64 is the platform to generate)
[33mâ•µ[0m[0m
[0m[0m
[0m[1m[32mTerraform has been successfully initialized![0m[32m[0m
[0m[32m
You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.[0m
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Plan)
[Pipeline] sh
+ terraform plan -var token=**** -var ssh_public_key=**** -out=tfplan
[0m[1mdata.template_file.cloudinit: Reading...[0m[0m
[0m[1mdata.template_file.cloudinit: Read complete after 0s [id=398fafd76ee8f170dc4971cf05af7f6e36e830aaee4d53744ae08389a303b30a][0m
[0m[1mmodule.analytics_vm.data.yandex_compute_image.my_image: Reading...[0m[0m
[0m[1mmodule.marketing_vm.data.yandex_compute_image.my_image: Reading...[0m[0m
[0m[1mmodule.marketing_vm.data.yandex_compute_image.my_image: Read complete after 0s [id=fd865v46cboopthn7u0k][0m
[0m[1mmodule.analytics_vm.data.yandex_compute_image.my_image: Read complete after 1s [id=fd865v46cboopthn7u0k][0m

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  [32m+[0m create[0m

Terraform will perform the following actions:

[1m  # module.analytics_vm.yandex_compute_instance.vm[0][0m will be created
[0m  [32m+[0m[0m resource "yandex_compute_instance" "vm" {
      [32m+[0m[0m allow_stopping_for_update = true
      [32m+[0m[0m created_at                = (known after apply)
      [32m+[0m[0m description               = "TODO: description; {{terraform managed}}"
      [32m+[0m[0m folder_id                 = (known after apply)
      [32m+[0m[0m fqdn                      = (known after apply)
      [32m+[0m[0m gpu_cluster_id            = (known after apply)
      [32m+[0m[0m hardware_generation       = (known after apply)
      [32m+[0m[0m hostname                  = "stage-web-stage-0"
      [32m+[0m[0m id                        = (known after apply)
      [32m+[0m[0m labels                    = {
          [32m+[0m[0m "owner"   = "s.pomelnikov"
          [32m+[0m[0m "project" = "analytics"
        }
      [32m+[0m[0m maintenance_grace_period  = (known after apply)
      [32m+[0m[0m maintenance_policy        = (known after apply)
      [32m+[0m[0m metadata                  = {
          [32m+[0m[0m "serial-port-enable" = "1"
          [32m+[0m[0m "user-data"          = <<-EOT
                #cloud-config
                users:
                  - name: ubuntu
                    groups: sudo
                    shell: /bin/bash
                    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
                    ssh_authorized_keys:
                      - ****
                package_update: true
                package_upgrade: false
                packages: ["vim","nginx"]
                runcmd:
                  - [ systemctl, enable, nginx ]
                  - [ systemctl, start, nginx ]
            EOT
        }
      [32m+[0m[0m name                      = "stage-web-stage-0"
      [32m+[0m[0m network_acceleration_type = "standard"
      [32m+[0m[0m platform_id               = "standard-v1"
      [32m+[0m[0m service_account_id        = (known after apply)
      [32m+[0m[0m status                    = (known after apply)
      [32m+[0m[0m zone                      = "ru-central1-a"

      [32m+[0m[0m boot_disk {
          [32m+[0m[0m auto_delete = true
          [32m+[0m[0m device_name = (known after apply)
          [32m+[0m[0m disk_id     = (known after apply)
          [32m+[0m[0m mode        = (known after apply)

          [32m+[0m[0m initialize_params {
              [32m+[0m[0m block_size  = (known after apply)
              [32m+[0m[0m description = (known after apply)
              [32m+[0m[0m image_id    = "fd865v46cboopthn7u0k"
              [32m+[0m[0m name        = (known after apply)
              [32m+[0m[0m size        = 10
              [32m+[0m[0m snapshot_id = (known after apply)
              [32m+[0m[0m type        = "network-hdd"
            }
        }

      [32m+[0m[0m network_interface {
          [32m+[0m[0m index              = (known after apply)
          [32m+[0m[0m ip_address         = (known after apply)
          [32m+[0m[0m ipv4               = true
          [32m+[0m[0m ipv6               = (known after apply)
          [32m+[0m[0m ipv6_address       = (known after apply)
          [32m+[0m[0m mac_address        = (known after apply)
          [32m+[0m[0m nat                = true
          [32m+[0m[0m nat_ip_address     = (known after apply)
          [32m+[0m[0m nat_ip_version     = (known after apply)
          [32m+[0m[0m security_group_ids = (known after apply)
          [32m+[0m[0m subnet_id          = (known after apply)
        }

      [32m+[0m[0m resources {
          [32m+[0m[0m core_fraction = 5
          [32m+[0m[0m cores         = 2
          [32m+[0m[0m memory        = 1
        }

      [32m+[0m[0m scheduling_policy {
          [32m+[0m[0m preemptible = true
        }
    }

[1m  # module.marketing_vm.yandex_compute_instance.vm[0][0m will be created
[0m  [32m+[0m[0m resource "yandex_compute_instance" "vm" {
      [32m+[0m[0m allow_stopping_for_update = true
      [32m+[0m[0m created_at                = (known after apply)
      [32m+[0m[0m description               = "TODO: description; {{terraform managed}}"
      [32m+[0m[0m folder_id                 = (known after apply)
      [32m+[0m[0m fqdn                      = (known after apply)
      [32m+[0m[0m gpu_cluster_id            = (known after apply)
      [32m+[0m[0m hardware_generation       = (known after apply)
      [32m+[0m[0m hostname                  = "develop-webs-0"
      [32m+[0m[0m id                        = (known after apply)
      [32m+[0m[0m labels                    = {
          [32m+[0m[0m "owner"   = "s.pomelnikov"
          [32m+[0m[0m "project" = "marketing"
        }
      [32m+[0m[0m maintenance_grace_period  = (known after apply)
      [32m+[0m[0m maintenance_policy        = (known after apply)
      [32m+[0m[0m metadata                  = {
          [32m+[0m[0m "serial-port-enable" = "1"
          [32m+[0m[0m "user-data"          = <<-EOT
                #cloud-config
                users:
                  - name: ubuntu
                    groups: sudo
                    shell: /bin/bash
                    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
                    ssh_authorized_keys:
                      - ****
                package_update: true
                package_upgrade: false
                packages: ["vim","nginx"]
                runcmd:
                  - [ systemctl, enable, nginx ]
                  - [ systemctl, start, nginx ]
            EOT
        }
      [32m+[0m[0m name                      = "develop-webs-0"
      [32m+[0m[0m network_acceleration_type = "standard"
      [32m+[0m[0m platform_id               = "standard-v1"
      [32m+[0m[0m service_account_id        = (known after apply)
      [32m+[0m[0m status                    = (known after apply)
      [32m+[0m[0m zone                      = "ru-central1-a"

      [32m+[0m[0m boot_disk {
          [32m+[0m[0m auto_delete = true
          [32m+[0m[0m device_name = (known after apply)
          [32m+[0m[0m disk_id     = (known after apply)
          [32m+[0m[0m mode        = (known after apply)

          [32m+[0m[0m initialize_params {
              [32m+[0m[0m block_size  = (known after apply)
              [32m+[0m[0m description = (known after apply)
              [32m+[0m[0m image_id    = "fd865v46cboopthn7u0k"
              [32m+[0m[0m name        = (known after apply)
              [32m+[0m[0m size        = 10
              [32m+[0m[0m snapshot_id = (known after apply)
              [32m+[0m[0m type        = "network-hdd"
            }
        }

      [32m+[0m[0m network_interface {
          [32m+[0m[0m index              = (known after apply)
          [32m+[0m[0m ip_address         = (known after apply)
          [32m+[0m[0m ipv4               = true
          [32m+[0m[0m ipv6               = (known after apply)
          [32m+[0m[0m ipv6_address       = (known after apply)
          [32m+[0m[0m mac_address        = (known after apply)
          [32m+[0m[0m nat                = true
          [32m+[0m[0m nat_ip_address     = (known after apply)
          [32m+[0m[0m nat_ip_version     = (known after apply)
          [32m+[0m[0m security_group_ids = (known after apply)
          [32m+[0m[0m subnet_id          = (known after apply)
        }

      [32m+[0m[0m resources {
          [32m+[0m[0m core_fraction = 5
          [32m+[0m[0m cores         = 2
          [32m+[0m[0m memory        = 1
        }

      [32m+[0m[0m scheduling_policy {
          [32m+[0m[0m preemptible = true
        }
    }

[1m  # module.vpc_dev.yandex_vpc_network.network[0m will be created
[0m  [32m+[0m[0m resource "yandex_vpc_network" "network" {
      [32m+[0m[0m created_at                = (known after apply)
      [32m+[0m[0m default_security_group_id = (known after apply)
      [32m+[0m[0m folder_id                 = (known after apply)
      [32m+[0m[0m id                        = (known after apply)
      [32m+[0m[0m labels                    = (known after apply)
      [32m+[0m[0m name                      = "develop"
      [32m+[0m[0m subnet_ids                = (known after apply)
    }

[1m  # module.vpc_dev.yandex_vpc_subnet.subnet["ru-central1-a"][0m will be created
[0m  [32m+[0m[0m resource "yandex_vpc_subnet" "subnet" {
      [32m+[0m[0m created_at     = (known after apply)
      [32m+[0m[0m folder_id      = (known after apply)
      [32m+[0m[0m id             = (known after apply)
      [32m+[0m[0m labels         = (known after apply)
      [32m+[0m[0m name           = "develop-ru-central1-a"
      [32m+[0m[0m network_id     = (known after apply)
      [32m+[0m[0m v4_cidr_blocks = [
          [32m+[0m[0m "10.0.1.0/24",
        ]
      [32m+[0m[0m v6_cidr_blocks = (known after apply)
      [32m+[0m[0m zone           = "ru-central1-a"
    }

[1m  # module.vpc_prod.yandex_vpc_network.network[0m will be created
[0m  [32m+[0m[0m resource "yandex_vpc_network" "network" {
      [32m+[0m[0m created_at                = (known after apply)
      [32m+[0m[0m default_security_group_id = (known after apply)
      [32m+[0m[0m folder_id                 = (known after apply)
      [32m+[0m[0m id                        = (known after apply)
      [32m+[0m[0m labels                    = (known after apply)
      [32m+[0m[0m name                      = "production"
      [32m+[0m[0m subnet_ids                = (known after apply)
    }

[1m  # module.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-a"][0m will be created
[0m  [32m+[0m[0m resource "yandex_vpc_subnet" "subnet" {
      [32m+[0m[0m created_at     = (known after apply)
      [32m+[0m[0m folder_id      = (known after apply)
      [32m+[0m[0m id             = (known after apply)
      [32m+[0m[0m labels         = (known after apply)
      [32m+[0m[0m name           = "production-ru-central1-a"
      [32m+[0m[0m network_id     = (known after apply)
      [32m+[0m[0m v4_cidr_blocks = [
          [32m+[0m[0m "10.1.1.0/24",
        ]
      [32m+[0m[0m v6_cidr_blocks = (known after apply)
      [32m+[0m[0m zone           = "ru-central1-a"
    }

[1m  # module.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-b"][0m will be created
[0m  [32m+[0m[0m resource "yandex_vpc_subnet" "subnet" {
      [32m+[0m[0m created_at     = (known after apply)
      [32m+[0m[0m folder_id      = (known after apply)
      [32m+[0m[0m id             = (known after apply)
      [32m+[0m[0m labels         = (known after apply)
      [32m+[0m[0m name           = "production-ru-central1-b"
      [32m+[0m[0m network_id     = (known after apply)
      [32m+[0m[0m v4_cidr_blocks = [
          [32m+[0m[0m "10.1.2.0/24",
        ]
      [32m+[0m[0m v6_cidr_blocks = (known after apply)
      [32m+[0m[0m zone           = "ru-central1-b"
    }

[1m  # module.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-d"][0m will be created
[0m  [32m+[0m[0m resource "yandex_vpc_subnet" "subnet" {
      [32m+[0m[0m created_at     = (known after apply)
      [32m+[0m[0m folder_id      = (known after apply)
      [32m+[0m[0m id             = (known after apply)
      [32m+[0m[0m labels         = (known after apply)
      [32m+[0m[0m name           = "production-ru-central1-d"
      [32m+[0m[0m network_id     = (known after apply)
      [32m+[0m[0m v4_cidr_blocks = [
          [32m+[0m[0m "10.1.3.0/24",
        ]
      [32m+[0m[0m v6_cidr_blocks = (known after apply)
      [32m+[0m[0m zone           = "ru-central1-d"
    }

[1mPlan:[0m 8 to add, 0 to change, 0 to destroy.
[0m[90m
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m

Saved the plan to: tfplan

To perform exactly these actions, run the following command to apply:
    terraform apply "tfplan"
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Apply)
[Pipeline] sh
+ terraform apply -auto-approve tfplan
[0m[1mmodule.vpc_dev.yandex_vpc_network.network: Creating...[0m[0m
[0m[1mmodule.vpc_prod.yandex_vpc_network.network: Creating...[0m[0m
[0m[1mmodule.vpc_prod.yandex_vpc_network.network: Creation complete after 2s [id=enp65q73dsmtntbrabp3][0m
[0m[1mmodule.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-a"]: Creating...[0m[0m
[0m[1mmodule.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-d"]: Creating...[0m[0m
[0m[1mmodule.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-b"]: Creating...[0m[0m
[0m[1mmodule.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-b"]: Creation complete after 0s [id=e2lg5pdsqm2b6b0kbh8e][0m
[0m[1mmodule.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-d"]: Creation complete after 1s [id=fl8d5an9v5nvbo1b8pob][0m
[0m[1mmodule.vpc_prod.yandex_vpc_subnet.subnet["ru-central1-a"]: Creation complete after 1s [id=e9b7608qu8qpjple32at][0m
[0m[1mmodule.vpc_dev.yandex_vpc_network.network: Creation complete after 4s [id=enp74vn7usj1i9h5gvkr][0m
[0m[1mmodule.vpc_dev.yandex_vpc_subnet.subnet["ru-central1-a"]: Creating...[0m[0m
[0m[1mmodule.vpc_dev.yandex_vpc_subnet.subnet["ru-central1-a"]: Creation complete after 0s [id=e9bjiqst5u17ocnt5uc4][0m
[0m[1mmodule.marketing_vm.yandex_compute_instance.vm[0]: Creating...[0m[0m
[0m[1mmodule.analytics_vm.yandex_compute_instance.vm[0]: Creating...[0m[0m
[0m[1mmodule.marketing_vm.yandex_compute_instance.vm[0]: Still creating... [10s elapsed][0m[0m
[0m[1mmodule.analytics_vm.yandex_compute_instance.vm[0]: Still creating... [10s elapsed][0m[0m
[0m[1mmodule.analytics_vm.yandex_compute_instance.vm[0]: Still creating... [20s elapsed][0m[0m
[0m[1mmodule.marketing_vm.yandex_compute_instance.vm[0]: Still creating... [20s elapsed][0m[0m
[0m[1mmodule.marketing_vm.yandex_compute_instance.vm[0]: Still creating... [30s elapsed][0m[0m
[0m[1mmodule.analytics_vm.yandex_compute_instance.vm[0]: Still creating... [30s elapsed][0m[0m
[0m[1mmodule.marketing_vm.yandex_compute_instance.vm[0]: Still creating... [40s elapsed][0m[0m
[0m[1mmodule.analytics_vm.yandex_compute_instance.vm[0]: Still creating... [40s elapsed][0m[0m
[0m[1mmodule.analytics_vm.yandex_compute_instance.vm[0]: Still creating... [50s elapsed][0m[0m
[0m[1mmodule.marketing_vm.yandex_compute_instance.vm[0]: Still creating... [50s elapsed][0m[0m
[0m[1mmodule.marketing_vm.yandex_compute_instance.vm[0]: Creation complete after 59s [id=fhmv3dcthr3h46ajd8qi][0m
[0m[1mmodule.analytics_vm.yandex_compute_instance.vm[0]: Creation complete after 1m0s [id=fhmmncr61k0scsnlfmfq][0m
[0m[1m[32m
Apply complete! Resources: 8 added, 0 changed, 0 destroyed.
[0m
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Destroy)
Stage "Destroy" skipped due to when conditional
[Pipeline] getContext
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Declarative: Post Actions)
[Pipeline] cleanWs
[WS-CLEANUP] Deleting project workspace...
[WS-CLEANUP] Deferred wipeout is used...
[WS-CLEANUP] done
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // withCredentials
[Pipeline] }
[Pipeline] // withEnv
[Pipeline] }
[Pipeline] // node
[Pipeline] End of Pipeline
Finished: SUCCESS
